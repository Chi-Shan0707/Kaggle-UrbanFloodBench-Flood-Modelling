import csv
import os
import sys
import numpy as np
import gc
from tqdm import tqdm

# ================= é…ç½®åŒºåŸŸ =================
SAMPLE_SUBMISSION_FILE = "sample_submission.csv"
PREDICTION_FILE = "final_submission.csv"
OUTPUT_FILE = "final_submission_filled.csv"
# ===========================================

class CompactPredictionStore:
    def __init__(self, pred_file):
        self.pred_file = pred_file
        self.offsets = {}      # Key -> Start Index in big array
        self.counts = {}       # Key -> Total count of predictions
        self.data_array = None # The giant flat array
        self.read_cursors = {} # Key -> How many we have read so far
        
        # 1. ç¬¬ä¸€éæ‰«æï¼šç»Ÿè®¡æ¯ä¸ª Key çš„æ•°æ®é‡ï¼Œé¢„è®¡ç®—åç§»é‡
        self._scan_offsets()
        
        # 2. ç¬¬äºŒéæ‰«æï¼šåŠ è½½æ•°æ®åˆ° numpy æ•°ç»„
        self._load_data()

    def _get_key(self, row):
        # Key: (model_id, event_id, node_type, node_id)
        # ä½¿ç”¨å…ƒç»„ä½œä¸ºå­—å…¸é”®
        return (int(row[1]), int(row[2]), int(row[3]), int(row[4]))

    def _scan_offsets(self):
        print("Phase 1/3: æ‰«æé¢„æµ‹æ–‡ä»¶ç»“æ„...")
        total_rows = 0
        
        with open(self.pred_file, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            header = next(reader) # Skip header
            
            # è‡ªåŠ¨æ£€æµ‹åˆ—ç´¢å¼•ï¼Œé˜²æ­¢åˆ—é¡ºåºå˜åŒ–
            try:
                # å¯»æ‰¾å¿…è¦çš„åˆ—ç´¢å¼•
                idx_map = {name: i for i, name in enumerate(header)}
                col_indices = [
                    idx_map['row_id'], 
                    idx_map['model_id'], 
                    idx_map['event_id'], 
                    idx_map['node_type'], 
                    idx_map['node_id'], 
                    idx_map['water_level']
                ]
            except KeyError as e:
                print(f"âŒ é”™è¯¯: é¢„æµ‹æ–‡ä»¶ç¼ºå°‘åˆ— {e}")
                sys.exit(1)

            # å¿«é€Ÿéå†ç»Ÿè®¡
            for row in tqdm(reader, desc="Counting", unit="rows"):
                key = (int(row[col_indices[1]]), int(row[col_indices[2]]), 
                       int(row[col_indices[3]]), int(row[col_indices[4]]))
                
                self.counts[key] = self.counts.get(key, 0) + 1
                total_rows += 1
        
        print(f"  - å‘ç° {total_rows} ä¸ªé¢„æµ‹ç‚¹ï¼Œæ¶‰åŠ {len(self.counts)} ä¸ªå”¯ä¸€èŠ‚ç‚¹åºåˆ—ã€‚")
        
        # è®¡ç®—åç§»é‡ (Cumulative Sum)
        current_offset = 0
        for key, count in self.counts.items():
            self.offsets[key] = current_offset
            current_offset += count
            
        self.total_capacity = current_offset
        
        # é¢„åˆ†é… Numpy æ•°ç»„ (float32 èŠ‚çœä¸€åŠå†…å­˜)
        # 5000ä¸‡ä¸ªæ•°æ®ç‚¹åªéœ€è¦ ~200MB å†…å­˜
        print(f"  - åˆ†é…å†…å­˜: {self.total_capacity * 4 / 1024 / 1024:.2f} MB")
        self.data_array = np.zeros(self.total_capacity, dtype=np.float32)

    def _load_data(self):
        print("Phase 2/3: åŠ è½½æ•°æ®åˆ°å†…å­˜...")
        
        # ä¸´æ—¶çš„å†™å…¥æŒ‡é’ˆ
        write_cursors = {k: 0 for k in self.counts.keys()}
        
        with open(self.pred_file, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            header = next(reader)
            
            idx_map = {name: i for i, name in enumerate(header)}
            # model, event, type, node, water
            c_m, c_e, c_t, c_n, c_w = (idx_map['model_id'], idx_map['event_id'], 
                                       idx_map['node_type'], idx_map['node_id'], 
                                       idx_map['water_level'])

            for row in tqdm(reader, total=self.total_capacity, desc="Loading", unit="rows"):
                key = (int(row[c_m]), int(row[c_e]), int(row[c_t]), int(row[c_n]))
                val = float(row[c_w])
                
                # è®¡ç®—åœ¨æ‰å¹³æ•°ç»„ä¸­çš„ç»å¯¹ä½ç½®
                # Pos = Start_Offset + Current_Write_Index
                abs_pos = self.offsets[key] + write_cursors[key]
                self.data_array[abs_pos] = val
                
                write_cursors[key] += 1
        
        # åˆå§‹åŒ–è¯»å–æŒ‡é’ˆä¾›åç»­ä½¿ç”¨
        self.read_cursors = {k: 0 for k in self.counts.keys()}
        del write_cursors
        gc.collect()

    def get_next_value(self, model, event, n_type, node):
        key = (model, event, n_type, node)
        
        # æ£€æŸ¥ Key æ˜¯å¦å­˜åœ¨
        if key not in self.offsets:
            return None
        
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰å‰©ä½™æ•°æ®
        cursor = self.read_cursors[key]
        if cursor >= self.counts[key]:
            return None # æ•°æ®ç”¨å®Œäº†
            
        # è·å–æ•°æ®
        abs_pos = self.offsets[key] + cursor
        val = self.data_array[abs_pos]
        
        # æŒ‡é’ˆå‰ç§»
        self.read_cursors[key] += 1
        return val

def fill_submission():
    if not os.path.exists(PREDICTION_FILE):
        print("âŒ æ‰¾ä¸åˆ°é¢„æµ‹æ–‡ä»¶")
        return

    # 1. åˆå§‹åŒ–ä¼˜åŒ–çš„æ•°æ®å­˜å‚¨
    store = CompactPredictionStore(PREDICTION_FILE)
    
    # 2. å¡«å……æ¨¡æ¿
    print("Phase 3/3: å¡«å……æ¨¡æ¿...")
    
    missing_count = 0
    nan_count = 0
    filled_count = 0
    
    with open(SAMPLE_SUBMISSION_FILE, 'r', encoding='utf-8') as f_in, \
         open(OUTPUT_FILE, 'w', newline='', encoding='utf-8') as f_out:
        
        reader = csv.DictReader(f_in)
        writer = csv.DictWriter(f_out, fieldnames=reader.fieldnames)
        writer.writeheader()
        
        # ä¼°ç®—è¡Œæ•°
        total_lines = None
        try:
            total_lines = sum(1 for _ in open(SAMPLE_SUBMISSION_FILE, 'rb')) - 1
        except: pass
        
        for row in tqdm(reader, total=total_lines, desc="Filling"):
            m = int(row['model_id'])
            e = int(row['event_id'])
            t = int(row['node_type'])
            n = int(row['node_id'])
            
            # ä»æˆ‘ä»¬çš„ç´§å‡‘å­˜å‚¨ä¸­è·å–ä¸‹ä¸€ä¸ªå€¼
            val = store.get_next_value(m, e, t, n)
            
            # ã€å…³é”®ä¿®æ”¹ 1ã€‘æ£€æŸ¥æ˜¯å¦ä¸º NaN (æ¨¡å‹ç®—ç‚¸äº†çš„æƒ…å†µ)
            if val is not None and (np.isnan(val) or np.isinf(val)):
                val = 0.0  # å¼ºåˆ¶ä¿®æ­£ä¸º 0
                nan_count += 1

            if val is not None:
                row['water_level'] = f"{val:.4f}"
                filled_count += 1
            else:
                # ã€å…³é”®ä¿®æ”¹ 2ã€‘å¦‚æœæ²¡æœ‰é¢„æµ‹å€¼ï¼Œæ˜¾å¼å¡« 0ï¼Œé˜²æ­¢ä¿ç•™æ¨¡æ¿é‡Œçš„ç©ºå€¼
                row['water_level'] = "0.0"
                missing_count += 1
            
            writer.writerow(row)

    print("\n" + "="*40)
    print(f"âœ… å®Œæˆï¼æ–‡ä»¶å·²ä¿å­˜: {OUTPUT_FILE}")
    print(f"ğŸ“Š ç»Ÿè®¡:")
    print(f"   - æˆåŠŸå¡«å……: {filled_count} è¡Œ")
    print(f"   - ç¼ºå¤±æ•°æ® (å·²è¡¥0): {missing_count} è¡Œ")
    print(f"   - æ¨¡å‹ NaN (å·²è¡¥0): {nan_count} è¡Œ")
    print("="*40)

if __name__ == "__main__":
    fill_submission()